---
title: "| \\vspace{5cm} \\Huge Car Price Prediction \n\n| and \n\n| \\ \\Huge Heart
  Disease Classification\n"
author: |
  | \vspace{0.5cm} \LARGE Hamid Hamidi, Thet Nyein, and Yanzhao Qian
  | \small Authors are in alphabetic order and have equal contribution.


output:
  pdf_document:
    extra_dependencies:
    - caption
    - bbm
    - xcolor
fontsize: 12
bibliography: references.bib
csl: citation_style.csl
---

\pagebreak


\newpage
\tableofcontents


\pagebreak

# Abstract

Generalized Linear Models (GLMs) are the extension of the ordinary linear regression models. GLMs enable us to use different distributions for the response with distinctive link functions @GLMintroduction. Here, we use two different data sets to show the broad applications of the GLMs in real-world problems, one of which is the "Car Price Prediction" @carprice, and the other is "Heart Failure prediction" @heartdata. In our analyses, we focus on model fitting and highlighting the most important variables instead of predicting desired outcomes and their accuracy. Our study of each data set is reported in its corresponding section. In the following, we discuss why we have chosen these data sets and provide a detailed description of our analyses along with the reasons and intuitions behind them. In both of these studies, all analyses were performed using the R programming language @R-base.

\pagebreak



# Car Price Prediction

## Introduction

One of the largest automotive markets in the world is the USA car market @carmarketUSA. Since 1982, when Honda invested in the USA car market, many other companies have been joining and competing in the USA car market resulting in foreign investment of more than $110 billion @carmarketUSA. These days, with skilled workers, local and governmental supports, a huge consumer market, and many other reasons, the USA car market is a primer market in the car industry. A new Chinese car company wants to join and compete in the USA car market. In the following, our goal is to identify significant variables affecting the car price and quantify their significance. These analyses are usually performed by a third party, such as a consulting company, or the business strategy division of the investing company. According to our findings, they can manipulate many variables, such as the car design, to have a better business strategy to enter the USA car market. These analyses can directly affect the success of billions of dollars investment. Consequently, our analyses are vital and should be detailed and valid.

We found out the car price (response) distribution is quite close to the Gamma distribution; therefore, we used the GLM with Gamma distribution and logarithmic link function to model the price of cars for distinctive variables. We also suspected that it might be possible to model the logarithm of price with Gaussian distribution and identity link function. However, the distribution of the logarithmic price is not close to the Normal distribution. Consequently, we only used the Gamma distribution with the logarithmic link function. We performed variable selection and selected the most reasonable model (details in the Statistical Analyses section). 

Using these analyses, we were able to identify several significant variables contributing to the car price, such as the car manufacturer (or the so-called brand of the car), the engine location (cars with rear engines are usually sports cars with higher prices), and the engine size (the bigger the higher the price).

Our data set, and consequently, our analyses have some limitations as well. For instance, electric cars are more than 2.5% of the USA car market @electricmarket but are not included in our data set. Additionally, luxury brands such as Rolls-Royce and Lincoln are missing. Furthermore, the majority of sports cars are missing in our data set, showing our limitation in analyzing the sports and luxury car price variables. In the following, we present a detailed description of our analysis, methods, and results. 





## Data Collection and exploration

The data were collected from the Kaggle website (kaggle.com), an online open-source community of data scientists and machine learning practitioners. One can easily access the online version of our data through @carprice. The data did not contain any missing values and was ready for analysis. However, we made minor changes and corrections in the data set. 

We removed the CAR ID column as it does not contain useful information for our analyses. Additionally, we changed the names of the cars into manufacturers' names. This way, the variable would represent the car brand (or manufacturer) reputation, which might have an impact on car price, instead of the model of the car, which is unique for most cars and would not impact the car price. 

Afterward, we tried to figure out the response distribution to use the appropriate link function and family of distribution @GLMintroduction. The distribution of the response resembles the Gamma distribution (Figure \ref{fig: Dist_response_car} (A)). We also visualized the logarithm of the response since it might be Gaussian (Figure \ref{fig: Dist_response_car} (B)). As one can see in Figure \ref{fig: Dist_response_car} (B), the logarithm of price does not resemble the Gaussian distribution. Therefore, we decided to only use the Gamma distribution with the log link function (See Statistical Analyses for more details). 

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.6\textwidth]{./Figures/car_price_dist.pdf}
    \caption{Distribution of the price. \\(A) The distribution of the response (B) The distribution of the logarithm of the response.}
    \label{fig: Dist_response_car}
\end{figure}

Figure \ref{fig: car_company} shows an overview of the manufacturers, the range of their cars' prices, and the fuel type of their productions. As it is obvious, electric cars are missing, and diesel cars are the minority. Moreover, as shown in Figure \ref{fig: car_company}, the car brands (or manufacturers) may affect the car price. For instance, cars from Porsche have a higher price compared to cars from Nissan or Mazda. Furthermore, we can see that famous sports car brands such as Ferrari and luxury manufacturers, such as Rolls-Royce and Lincoln, are missing. 

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth]{./Figures/car_company.pdf}
    \caption{The range of car price in different brands and fuel types}
    \label{fig: car_company}
\end{figure}


In Appendix Figure \ref{fig: engine_size}, we can see that the car price is correlated with engine size; however, it might not be a linear correlation. Also, what stands out in this figure is the general growth of the engine size with increasing the number of Cylinders, and also, the response will rise with increasing any of them.  


We also suspected that there might be some trends with the quadratic increase of numerical variables. Therefore, we investigated these patterns. For instance, in Appendix Figure \ref{fig: wheelbase}, we divided the wheelbase^[In cars, the wheelbase is the distance between the front and rear wheels @Wheelbase.] of cars into four groups and visualized the trend between the wheelbase and the response. As this figure shows, the car price is not growing linearly with the increase of the wheelbase. Hence, we included quadratic terms in our statistical model as well. In the next section, the details of our statistical analyses are described. 



## Statistical Analyses

## Conclusion


\pagebreak

# Heart Disease Classification

## Introduction

Heart Failure, also known as congestive heart failure, can broadly be defined as a condition that happens when the heart cannot supply the body's need for Oxygen and blood @HeartFailure. According to the latest annual statistical report from the American Heart Association and the National Institutes of Health, about 6.2 million adults in the United States have heart failure @virani2020heart. Furthermore, in 2018, heart failure was mentioned on 379,800 death certificates (13.4%) @virani2020heart and cost about $30 billion annually @benjamin2019heart. 

This suggests that identifying the core health behaviors and risk factors influencing heart failure is critical not only for our community health but also for our economy. Therefore, we decided to analyze the "Heart Failure prediction" data set @heartdata to find variables playing a key role in heart failure. As the response in our data set is binary (0 or 1), we used logistic regression to model the probability of having heart failure. Moreover, we performed variable selection to select the best model and determine major factors in heart failure (details in the Statistical Analyses section).

This study has generally revealed causal factors in heart failure such as sex, exercise angina (a type of chest pain during performing exercises), distinctive types of chest pain, and squared Cholesterol level. 

The generalisability of our results is subject to certain limitations. For instance, our data set does not cover younger generations (less than 28 years old), which will cause the analyses to be biased toward older ages. Another issue that was not addressed in this study was the mortality of the patients. This might not seem arguable at first look. However, many patients with asymptomatic chest pain might have lived without any critical problems throughout their lives, and patients with other types of chest pain might have faced devastating situations. This might cause our findings to be questionable from different perspectives. Overall, our study concluded significant risk factors in heart failure. The following chapters are a detailed description of our analyses, methods, and results.



## Data Collection and exploration

The data were collected from the Kaggle website (kaggle.com), an online open-source community of data scientists and machine learning practitioners. One can easily access the online version of our data through @heartdata. The data contain 918 subjects with 11 covariates and one binary response (Heart Failure or not). The data do not have any missing values and are ready for analysis.

Figure \ref{fig: age_heart_sex} represents the distribution of age in samples for their sexuality and heart condition. As it is obvious, the number of male samples is dramatically higher than female samples indicating whether our data set is biased or males have more heart failure than females. According to two distinctive independent studies, men have more incidents of heart failure, which is consistent with our data set @stromberg2003gender, @mehta2006gender. Another observation in Figure \ref{fig: age_heart_sex} is that the range of the age starts from 28, showing that our data set does not contain younger generations, and our analysis is not valid for younger ages.


As shown in Appendix Figure \ref{fig: chest_pain_exercise_angina} (A), asymptomatic chest pain type is more frequent in men and women, and also samples with asymptomatic chest pain type are more exposed to heart failure. This observation is not surprising because patients with heart failure may show symptoms @Heartchest. Appendix Figure \ref{fig: chest_pain_exercise_angina} (B) presents that samples with exercise-induced angina^[Angina is a type of chest pain caused by reduced blood flow to the heart @Angina.] are more exposed to heart failure in both sex groups.

In Appendix Figure \ref{fig: oldpeak}, we can see that the patients with heart problems have higher oldpeak^[ST depression induced by exercise relative to rest @palaniappan2008intelligent.], and as one might suspect, the heart failure might be affected by the quadratic of the oldpeak as well. Therefore, we decided to include quadratic forms of numerical variables in our modeling as well. A detailed description of the main findings, together with our conclusion, is provided in the next chapters. 

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth]{./Figures/age_heart_sex.pdf}
    \caption{The distribution of the age of the samples with respect to their sex and heart condition}
    \label{fig: age_heart_sex}
\end{figure}


## Statistical Analyses

### Three GLM models and stepwise AIC selection

To begin with, let's assign each feature with a variable name.




\begin{tabular}{ |p{2.3cm}|p{3cm}|p{9cm}|p{0.7cm}|  }
 \hline
 \multicolumn{4}{|c|}{Data Dictionary} \\
 \hline
 Variable Name & Definition & Explanation & Vari- able name\\
 \hline
    Age   &   age of the patient    &   years   &   $x_1$   \\ 
   Sex   &   sex of the patient   &   M: Male, F: Female   &   $x_2$   \\ 
   ChestPainType   &   chest pain type   &   TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic   &   $x_3$   \\ 
   RestingBP   &   resting blood pressure    &   mm Hg   &   $x_4$   \\ 
   Cholesterol   &   serum cholesterol   &   1: if FastingBS > 120 mg/dl, 0: otherwise   &   $x_5$   \\ 
   FastingBS   &   fasting blood sugar   &   Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria   &   $x_6$   \\ 
   RestingECG   &   resting electrocardiogram results   &   Numeric value between 60 and 202   &   $x_7$   \\ 
   MaxHR   &   maximum heart rate achieved   &   Y: Yes, N: No   &   $x_8$   \\ 
   ExerciseAngina   &    exercise-induced angina    &   Numeric value measured in depression   &   $x_9$   \\ 
   Oldpeak   &   oldpeak = ST    &   Numeric value measured in depression   &   $x_{10}$   \\ 
   ST\_Slope   &   the slope of the peak exercise ST segment    &   Up: upsloping, Flat: flat, Down: downsloping   &   $x_{11}$   \\ 
   HeartDisease   &   output class   &   1: heart disease, 0: Normal   &   $y$   \\ 

 \hline
 \end{tabular}
 
#### Modeling All the Main Covariates
 
 Since the response is having heart disease or not, it is a binomial distributed response. So it is suggested we could use a logistic regression model.
 At first, we created a logistic model of all the main effects, and the formula of the model is
$$
\log \frac{p}{1-p} = \beta_0 + \sum_{i=1}^{11} \beta_i x_i
$$
where $p$ is the probability to get the heart disease, and $\beta_i, i = 1,2, \dots, 11$ is the coefficients of the parameter, $\beta_0$ is the intercept. 
So we fitted the model, and got the estimated parameters shown in the table below. 


\begin{center}
\begin{tabular}{|l|c|c|c|}
 \hline
 & Estimate & Pr(>|z|) & significance\\ 
 \hline
(Intercept) & -1.16 & 0.411 & not significant\\ 
Age & 0.0166 & 0.21 & not significant\\ 
SexM & 1.47 & 1.6e-07 & ***\\ 
ChestPainTypeATA & -1.83 & 2.03e-08 & ***\\ 
ChestPainTypeNAP & -1.69 & 2.34e-10 & ***\\ 
ChestPainTypeTA & -1.49 & 0.00058 & *\\ 
RestingBP & 0.00419 & 0.485 & not significant\\ 
Cholesterol & -0.00411 & 0.000154 & *\\ 
FastingBS & 1.14 & 3.59e-05 & **\\ 
RestingECGNormal & -0.177 & 0.515 & not significant\\ 
RestingECGST & -0.269 & 0.443 & not significant\\ 
MaxHR & -0.00429 & 0.393 & not significant\\ 
ExerciseAnginaY & 0.9 & 0.000231 & *\\ 
Oldpeak & 0.381 & 0.00131 & .\\ 
ST\_SlopeFlat & 1.45 & 0.000703 & *\\ 
ST\_SlopeUp & -0.994 & 0.0272 & .\\
 \hline
 \end{tabular} 
\end{center}
As we can see from the table, there are some variables that are not significant. We should drop some variables to make the model simpler. We choose backward step selection to do so. 
And we get our estimated paraeters shown in the table below.

\begin{center}
\begin{tabular}{|l|c|c|c|}
 \hline
 & Estimate & Pr(>|z|) & significance\\ 
 \hline
(Intercept) & -1.72 & 0.0436 & .\\ 
Age & 0.0231 & 0.0518 & not significant\\ 
SexM & 1.47 & 1.36e-07 & ***\\ 
ChestPainTypeATA & -1.86 & 8.89e-09 & ***\\ 
ChestPainTypeNAP & -1.72 & 6.13e-11 & ***\\ 
ChestPainTypeTA & -1.49 & 0.000494 & *\\ 
Cholesterol & -0.00398 & 0.000106 & *\\ 
FastingBS & 1.13 & 3.41e-05 & **\\ 
ExerciseAnginaY & 0.936 & 8.21e-05 & **\\ 
Oldpeak & 0.377 & 0.00121 & .\\ 
ST\_SlopeFlat & 1.46 & 0.000654 & *\\ 
ST\_SlopeUp & -1.03 & 0.0211 & .\\
 \hline
 \end{tabular} 
\end{center}
 
From the table, we can see all the variables are significant now.  here is the **model 1** formula.

$$
\begin{aligned}
\log \frac{\hat p}{1-\hat p}& = -1.7  + 0.023 \text{Age} + 1.5 \text{SexM} - 1.9 \text{ChestPainTypeATA}\\ 
&- 1.7 \text{ChestPainTypeNAP} - 1.5\text{ChestPainTypeTA} - 0.0040 \text{Cholesterol}\\
&- 1.1 \text{FastingBS }+0.94 \text{ExerciseAnginaY }+ 0.38 \text{Oldpeak}\\
& + 1.5 \text{ST\_slopeFlat} - 1.0 \text{ST\_slopeUp}\\
\end{aligned}
$$


We need to test if the selected model is good enough to represent the origin mode. We did log-likelihood ratio test for the step-wise selected variables to see if the drop out is good. 
The null hypothesis is 

$$
H_0: \beta_\text{RestingBP} = \beta_\text{RestingECG} = \beta_\text{MaxHR} =0 \text{  v.s } H_1: \text{At least one of these parameters not 0}
$$

We used the formula to get LLR statistic as
$$
LLR = 2(\ell(\text{full model}) - \ell(\text{reduced mode})) = 2\times(-297.0925 +297.9042) =0.804
$$
with degrees of freedom of 4. So we can calculate the p-value is 0.8046016, which is very high. So we cannot reject $H_0$. So we can accept the reduced model.


#### Modeling the Square of Numerical Variables 

Now we investigated the square of the numerical variables. We did this approach because response may have some quadratic effect of the numerical variables, and square of categorical variables do not make any difference. The odds model is 

$$
\log \frac{p}{1-p} = \beta_0 + \sum_{i=1}^{11} \beta_i x_i + \beta_{12}x_1 ^2 + \beta_{42}x_4^2 + \beta_{72}x_7^2 + \beta_{102}x_{10}^2
$$
The model is fitted by R. And all the parameters and significance of them are demonstrated in the table below.

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
  & Estimate & Pr(>|z|) & significance\\
\hline
(Intercept) & 3.34e+00 & 4.96e-01 & not significant\\
Age & -2.90e-02 & 7.86e-01 & not significant\\
SexM & 1.49e+00 & 2.00e-07 & ***\\
ChestPainTypeATA & -1.71e+00 & 2.00e-07 & ***\\
ChestPainTypeNAP & -1.68e+00 & 0.00e+00 & ***\\
ChestPainTypeTA & -1.38e+00 & 2.31e-03 & .\\
RestingBP & -2.86e-02 & 5.47e-01 & not significant\\
Cholesterol & -1.16e-02 & 2.87e-05 & **\\
FastingBS & 1.08e+00 & 1.73e-04 & *\\
RestingECGNormal & -1.65e-01 & 5.53e-01 & not significant\\
RestingECGST & -2.96e-01 & 4.10e-01 & not significant\\
MaxHR & -2.29e-02 & 5.81e-01 & not significant\\
ExerciseAnginaY & 1.08e+00 & 2.09e-05 & **\\
Oldpeak & -3.04e-01 & 3.33e-01 & not significant\\
ST\_SlopeFlat & 1.77e+00 & 1.12e-04 & *\\
ST\_SlopeUp & -7.29e-01 & 1.26e-01 & not significant\\
I(MaxHR\textasciicircum{}2) & 7.45e-05 & 6.24e-01 & not significant\\
I(Age\textasciicircum{}2) & 4.72e-04 & 6.37e-01 & not significant\\
I(Oldpeak\textasciicircum{}2) & 2.67e-01 & 1.96e-02 & .\\
I(RestingBP\textasciicircum{}2) & 1.20e-04 & 4.95e-01 & not significant\\
I(Cholesterol\textasciicircum{}2) & 2.17e-05 & 2.22e-03 & .\\
\hline
\end{tabular}
\end{center}

But there are still some useless covariates in the model. To simplify the model, a backward step-wise selection was used to the square of numerical model. The parameters and their significance of selected model are shown in the table.

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
  & Estimate & Pr(>|z|) & significance\\
  \hline
(Intercept) & -1.05e+00 & 1.25e-01 & not significant\\
SexM & 1.50e+00 & 1.00e-07 & ***\\
ChestPainTypeATA & -1.72e+00 & 1.00e-07 & ***\\
ChestPainTypeNAP & -1.68e+00 & 0.00e+00 & ***\\
ChestPainTypeTA & -1.37e+00 & 1.98e-03 & .\\
Cholesterol & -1.18e-02 & 9.90e-06 & ***\\
FastingBS & 1.05e+00 & 2.10e-04 & *\\
ExerciseAnginaY & 1.02e+00 & 1.92e-05 & **\\
ST\_SlopeFlat & 1.74e+00 & 1.20e-04 & *\\
ST\_SlopeUp & -7.23e-01 & 1.22e-01 & not significant\\
I(Age\textasciicircum{}2) & 2.22e-04 & 4.77e-02 & .\\
I(Oldpeak\textasciicircum{}2) & 1.70e-01 & 2.21e-04 & *\\
I(Cholesterol\textasciicircum{}2) & 2.24e-05 & 1.44e-03 & .\\
\hline
\end{tabular}
\end{center}

We now apply Log-likelihood ratio test to get
$$
LLR = 2(\ell(\text{full model}) - \ell(\text{reduced mode})) = 2\times(-286.9175 +288.5404) =3.245983
$$
with 4 degrees of freedom. So the p-value is 0.9179859, which means the reduction is highly possible. The formula for **model 2** is

$$
\begin{aligned}
\log \frac{\hat p}{1-\hat p} = & -1.1  + 1.5 \text{SexM} -1.7 \text{ChestPainTypeATA}\\ 
&- 1.7 \text{ChestPainTypeNAP} - 1.4 \text{ChestPainTypeTA} - 0.0012 \text{Cholesterol}\\
&+ 1.0 \text{FastingBS} + 1.0 \text{ExerciseAnginaY} \\
&+ 1.7 \text{ST\_slopeFlat} - 7.2 \text{ST\_slopeUp} \\
& + 2.2 \times 10^{-4} \text{Age}^2 + 1.7 \text{Oldpeak} ^2 + 2.2 \times 10^{-5} \text{Cholesterol} ^2
\end{aligned}
$$

#### Modeling the Interaction of Numerical
We also modeled the interaction between numerical terms, and the model is 
$$
\log \frac{p}{1-p} = \beta_0 + \sum_{i=1}^{11} \beta_i x_i + \sum_{i \in \mathcal{N}}
\beta_{i2}x_i ^2 + \sum_{i \in \mathcal{N}}\sum_{j \in \mathcal{N}}\beta_{ij}x_i x_j
$$
where $\mathcal N = \{1, 4, 7, 10\}$, which means the set of numerical variables. We also did backward stepwise selection to reduce the model, ane got all the coefficients demonstrated in the table below 

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
  & Estimate & Pr(>|z|) & significance\\
  \hline
(Intercept) & -1.950000 & 1.29e-01 & not significant\\
Age & 0.021300 & 9.17e-02 & not significant\\
SexM & 1.420000 & 4.00e-07 & ***\\
ChestPainTypeATA & -1.810000 & 0.00e+00 & ***\\
ChestPainTypeNAP & -1.660000 & 0.00e+00 & ***\\
ChestPainTypeTA & -1.450000 & 9.97e-04 & *\\
Cholesterol & -0.003860 & 2.71e-04 & *\\
FastingBS & 1.200000 & 1.44e-05 & **\\
MaxHR & 0.000751 & 8.97e-01 & not significant\\
ExerciseAnginaY & 0.993000 & 6.60e-05 & **\\
Oldpeak & 0.668000 & 3.73e-01 & not significant\\
ST\_SlopeFlat & 1.810000 & 7.59e-05 & **\\
ST\_SlopeUp & -0.732000 & 1.20e-01 & not significant\\
I(Oldpeak\textasciicircum{}2) & 0.324000 & 6.27e-03 & .\\
MaxHR:Oldpeak & -0.007750 & 1.42e-01 & not significant\\
\hline
\end{tabular}
\end{center}

Similarly, we did a Log-likelihood ratio test to the reduction, and get
$$
LLR = 2(\ell(\text{full model}) - \ell(\text{reduced mode})) = 2\times(-288.2246 +292.3055) =8.161785
$$
with 11 degrees of freedom. So the p-value is 0.699, which means the reduction is acceptable. And the formula for **model 3** is

$$
\begin{aligned}
\log \frac{\hat p}{1- \hat p} = & -1.9  + 0.02 \text{Age} + 1.4 \text{SexM} -1.8 \text{ChestPainTypeATA}\\ 
& - 1.7 \text{ChestPainTypeNAP} - 1.4 \text{ChestPainTypeTA} - 0.0039 \text{Cholesterol}\\
& + 1.19 \text{FastingBS} + 7.5 \times 10^{-4} \text{MaxHR} + 1.0 \text{ExerciseAnginaY} \\
& + 1.8 \text{ST\_slopeFlat} - 0.73 \text{ST\_slopeUp} \\
& +0.32 \text{Oldpeak} ^2 \\
& -7.7 \times 10^{-3} \text{MaxHR} \times \text{Oldpeak}         
\end{aligned}
$$

### Analysis of the Three Models

#### Receiver operating characteristics (ROC) 
graphs are useful for organizing classifiers and visualizing their performance @ROC. 
ROC curves demonstrate true positive rate on the Y axis and false positive rate on the X axis with different decision criterion. If the ROC curve can reach left-up corner in the graph, false positive rate reach zero and true positive rate of 1, which means the model has an perfect classification @scikit.  Therefore, the more for CROC curve lean to the right-up corner, the better performance of the model has. The ROC curves for three models are shown in the plot\ref{fig: roc_curve}.


\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.7\textwidth]{./Figures/roc_plot.png}
    \caption{The ROC curves for Three Models}
    \label{fig: roc_curve}
\end{figure}

In the ROC graph \lef{fig: roc_curve}, three models have almost coincided into the same curve, which means three models have similar performance. To further investigate, AUC, the area under the ROC curve was also calculated to compare three models. AUC for model 1 is 0.932, for model 2 and 3 are 0.9368, 0.936. Three AUC are very similar, as we expected.

#### Pseudo-R square 

Pseudo-R square sometimes is used to measure the explanatory of the model. The McFadden Pseudo-R square is stated below @walker2016nine. 

$$
R_{\mathrm{MF}}^{2}=1-\frac{\operatorname{LL}(\text { Full })}{\operatorname{LL}(\text { Null })}
$$
The McFadden Pseudo-R square is between 0 and 1. The higher value Pseudo-R square is, the higher explanatory the model have. 
McFadden Pseudo-R square of the three models are calculated as 0.537, 0.543 and 0.528, among which model 2 has the highest value.

#### Deviance Goodness of Fit

Finally, we calculated the residual deviance for the three models. We have the null hypothesis that is $H_0$: the model fits well versus $H_1$: the model fits poorly.

Model 1 has residual deviance of 595.81 on 906 degrees of freedom. Thus the p-value is near to 1. That means model 1 is a good fit. Model 2 has the deviance of 577.08  on 905  degrees of freedom. Thus p-value is approximate 1. Model 3 has 584.61 on 903 degrees of freedom with p-value 1. Among the deviance of three models, model 2 has the least value.

#### The Akaike information criterion (AIC)
AIC is a popular method for comparing the adequacy of multiple, possibly non-nested models.The objective of AIC model selection is to estimate the information loss when the probability distribution $f$
associated with the true (generating) model is approximated by probability distribution $g$, associated with the
model that is to be evaluated @wagenmakers2004aic. 
For choosing a model from a sequence of model candidates $M_i$, $i = 1,2,\dots K$. The AIC is defined as
$$
\text{AIC}_i = -2 \log L_i + 2 V_i
$$
where $L_i$, the maximum likelihood for the candidate model i, is determined by adjusting the $V_i$ free parameters in such a way as to maximize the probability that the candidate model has generated the observed data @wagenmakers2004aic. Akaike showed that the model with lowest information loss has the lowest AIC @Akaike1998.     

The AIC for three models are 619.81, 603.08 and 614.61, respectly, among which model 2 has the lowest value.

#### Binned Residual Plots
Residual plots, residual-versus-fitted values, is commonly used in linear regression models to diagnosis the model, such as to assess the validity of assumptions, to identify features not captured by the model, and to find outliers. But it is not useful for a binomial outcome model, because the response is discrete. Binned residual plots can be used to assess the binomial outcome model like logistic regression. 

To construct a binned residual plot, Data are split into bins containing equal numbers of observations, and the average residual is plotted against the average predicted probability for each bin. For each bin,
approximate 95% confidence limits are $\pm 2 \sqrt{p(1-p)/n}$, estimated by using standard deviation of residuals in each bin. If a model is good, most(95%) of the points are expected to lie between two confidence limits @kasza2015stata. 

The binned residual of model 2 is shown in following graph\ref{fig: Binned 2}, the plots for other two models are shown in the appendix. Thus, We can conclude that three models are good, since most of the residuals lie within the confidence limits.

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.75\textwidth]{./Figures/binned plot 2.png}
    \caption{Binned Residuals Plots for Model 2}
    \label{fig: Binned 2}
\end{figure}



## Conclusion


\pagebreak
# Appendix

## Appendix Figures


\renewcommand{\figurename}{Appendix Figure}
\setcounter{figure}{0}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.75\textwidth]{./Figures/engine_size.pdf}
    \caption{The correlation between engine size and price for distinctive number of Cylinders}
    \label{fig: engine_size}
\end{figure}


\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.35\textwidth]{./Figures/wheel_base_groups.pdf}
    \caption{The quadratic increase of price with different groups of wheelbase.}
    \label{fig: wheelbase}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./Figures/chest_pain_exercise_angina.pdf}
    \caption{The number of samples with different chest pain type and exercise angina. (A) The distinctive chest pain types in different genders. TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic (B) Number of samples with exercise angina with respect to their sex. Y: Yes, with exercise angina and N: No, without exercise angina.}
    \label{fig: chest_pain_exercise_angina}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.6\textwidth]{./Figures/old_peak.pdf}
    \caption{The relationship between having heart disease and the oldpeak.}
    \label{fig: oldpeak}
\end{figure}


\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.75\textwidth]{./Figures/binned plot 1.png}
    \caption{Binned Residuals Plots for Model 1}
    \label{fig: Binned 1}
\end{figure}
\pagebreak

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.75\textwidth]{./Figures/binned plot 3.png}
    \caption{Binned Residuals Plots for Model 3}
    \label{fig: Binned 3}
\end{figure}

# References

